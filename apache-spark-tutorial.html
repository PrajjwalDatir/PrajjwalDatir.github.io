<!DOCTYPE html>
<html lang="en" prefix="og: http://ogp.me/ns# fb: https://www.facebook.com/2008/fbml">
<head>
    <title>Donne Martin</title>
    <!-- Using the latest rendering mode for IE -->
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="description" content="">
    <meta name="author" content="">




        <meta name="author" content="Donne Martin" />

    <!-- Open Graph tags -->
        <meta property="og:site_name" content="Donne Martin" />
        <meta property="og:type" content="website"/>
        <meta property="og:title" content="Donne Martin"/>
        <meta property="og:url" content="."/>
        <meta property="og:description" content="Donne Martin"/>


    <!-- Bootstrap -->
        <link rel="stylesheet" href="./theme/css/bootstrap.min.css" type="text/css"/>

    <link href="./theme/css/pygments/monokai.css" rel="stylesheet">





    <!-- Custom CSS -->
    <link href="./theme/css/agency.css" rel="stylesheet">
    <link href="./theme/css/custom.css" rel="stylesheet">

    <!-- Custom Fonts -->
    <link href="./theme/font-awesome/css/font-awesome.min.css" rel="stylesheet" type="text/css">
    <link href="https://fonts.googleapis.com/css?family=Montserrat:400,700" rel="stylesheet" type="text/css">
    <link href='https://fonts.googleapis.com/css?family=Kaushan+Script' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Droid+Serif:400,700,400italic,700italic' rel='stylesheet' type='text/css'>
    <link href='https://fonts.googleapis.com/css?family=Roboto+Slab:400,100,300,700' rel='stylesheet' type='text/css'>

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="https://oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="https://oss.maxcdn.com/libs/respond../theme/js/1.4.2/respond.min.js"></script>
    <![endif]-->

</head><body id="page-top" class="index">


<!-- Banner -->
<!-- End Banner -->

<div class="container">
    <div class="row">
        <div class="col-lg-12">
    <nav class="navbar navbar-default navbar-fixed-top" style="background-color: #000">
        <div class="container">
            <!-- Brand and toggle get grouped for better mobile display -->
            <div class="navbar-header page-scroll">
                <button type="button" class="navbar-toggle" data-toggle="collapse" data-target="#bs-example-navbar-collapse-1">
                    <span class="sr-only">Toggle navigation</span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                    <span class="icon-bar"></span>
                </button>
                <a class="navbar-brand page-scroll" href=".">Donne Martin</a>
            </div>

            <!-- Collect the nav links, forms, and other content for toggling -->
            <div class="collapse navbar-collapse" id="bs-example-navbar-collapse-1">
                <ul class="nav navbar-nav navbar-right">
                    <li class="hidden">
                        <a href="#page-top"></a>
                    </li>
                    <li>
                        <a class="page-scroll" href="./#likes">Likes</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="./#portfolio">GitHub</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="./#about">About</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="./#contact">Contact</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="./archives">Blog</a>
                    </li>
                    <li>
                        <a class="page-scroll" href="http://donnemartin.com/viz/">Viz</a>
                    </li>
                </ul>
            </div>
            <!-- /.navbar-collapse -->
        </div>
        <!-- /.container-fluid -->
    </nav>    <section id="content" class="section-top-padding">
        <article class="article-top-padding">
            <h1>
                <a href="./apache-spark-tutorial.html"
                   rel="bookmark"
                   title="Permalink to Apache Spark Tutorial">
                    Apache Spark Tutorial
                </a>
            </h1>
            <i><time datetime="2016-06-19T00:00:00-04:00"> Sun 19 June 2016</time></i>
            <div class="entry-content">
                <div class="panel">
                <br/>
                </div>
                <div class="container">
                    <br/>
                    <img class="img-responsive" src="http://i.imgur.com/Ud8lvyp.png">
                </div>
                <hr class="featurette-divider">
                <h1>Spark</h1>
<ul>
<li>IPython Notebook Setup</li>
<li>Python Shell</li>
<li>DataFrames</li>
<li>RDDs</li>
<li>Pair RDDs</li>
<li>Running Spark on a Cluster</li>
<li>Viewing the Spark Application UI</li>
<li>Working with Partitions</li>
<li>Caching RDDs</li>
<li>Checkpointing RDDs</li>
<li>Writing and Running a Spark Application</li>
<li>Configuring Spark Applications</li>
<li>Streaming</li>
<li>Streaming with States</li>
<li>Broadcast Variables</li>
<li>Accumulators</li>
</ul>
<h2>IPython Notebook Setup</h2>
<p>The <a href="https://github.com/donnemartin/dev-setup">dev-setup</a> repo contains scripts to install Spark and to automate the its integration with IPython Notebook through the <a href="https://github.com/donnemartin/dev-setup/blob/master/aws.sh">pydata.sh script</a>.</p>
<p>You can also follow the instructions provided <a href="http://ramhiser.com/2015/02/01/configuring-ipython-notebook-support-for-pyspark/">here</a> to configure IPython Notebook Support for PySpark with Python 2.</p>
<p>To run Python 3 with Spark 1.4+, check out the following posts on <a href="http://stackoverflow.com/questions/30279783/apache-spark-how-to-use-pyspark-with-python-3">Stack Overflow</a> or <a href="http://www.reddit.com/r/datascience/comments/3ar1bd/continually_updated_data_science_python_notebooks/">Reddit</a>.</p>
<h2>Python Shell</h2>
<p>Start the pyspark shell (REPL):</p>
<div class="highlight"><pre><span class="n">pyspark</span>
</pre></div>


<p>View the spark context, the main entry point to the Spark API:</p>
<div class="highlight"><pre><span class="n">sc</span>
</pre></div>


<h2>DataFrames</h2>
<p>From the following <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">reference</a>:</p>
<p>A DataFrame is a distributed collection of data organized into named columns. It is conceptually equivalent to a table in a relational database or a data frame in R/Python, but with richer optimizations under the hood.</p>
<p>Create a DataFrame from JSON files on S3:</p>
<div class="highlight"><pre><span class="n">users</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="s">&quot;s3n://path/to/users.json&quot;</span><span class="p">,</span> <span class="s">&quot;json&quot;</span><span class="p">)</span>
</pre></div>


<p>Create a new DataFrame that contains “young users” only:</p>
<div class="highlight"><pre><span class="n">young</span> <span class="o">=</span> <span class="n">users</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">users</span><span class="o">.</span><span class="n">age</span><span class="o">&lt;</span><span class="mi">21</span><span class="p">)</span>
</pre></div>


<p>Alternatively, using Pandas-like syntax:</p>
<div class="highlight"><pre><span class="n">young</span> <span class="o">=</span> <span class="n">users</span><span class="p">[</span><span class="n">users</span><span class="o">.</span><span class="n">age</span><span class="o">&lt;</span><span class="mi">21</span><span class="p">]</span>
</pre></div>


<p>Increment everybody’s age by 1:</p>
<div class="highlight"><pre><span class="n">young</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="n">young</span><span class="o">.</span><span class="n">name</span><span class="p">,</span> <span class="n">young</span><span class="o">.</span><span class="n">age</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
</pre></div>


<p>Count the number of young users by gender:</p>
<div class="highlight"><pre><span class="n">young</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;gender&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>


<p>Join young users with another DataFrame called logs:</p>
<div class="highlight"><pre><span class="n">young</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">logs</span><span class="p">,</span> <span class="n">logs</span><span class="o">.</span><span class="n">userId</span> <span class="o">==</span> <span class="n">users</span><span class="o">.</span><span class="n">userId</span><span class="p">,</span> <span class="s">&quot;left_outer&quot;</span><span class="p">)</span>
</pre></div>


<p>Count the number of users in the young DataFrame:</p>
<div class="highlight"><pre><span class="n">young</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;young&quot;</span><span class="p">)</span>
<span class="n">context</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT count(*) FROM young&quot;</span><span class="p">)</span>
</pre></div>


<p>Convert Spark DataFrame to Pandas:</p>
<div class="highlight"><pre><span class="n">pandas_df</span> <span class="o">=</span> <span class="n">young</span><span class="o">.</span><span class="n">toPandas</span><span class="p">()</span>
</pre></div>


<p>Create a Spark DataFrame from Pandas:</p>
<div class="highlight"><pre><span class="n">spark_df</span> <span class="o">=</span> <span class="n">context</span><span class="o">.</span><span class="n">createDataFrame</span><span class="p">(</span><span class="n">pandas_df</span><span class="p">)</span>
</pre></div>


<p>Given the Spark Context, create a SQLContext:</p>
<div class="highlight"><pre><span class="kn">from</span> <span class="nn">pyspark.sql</span> <span class="kn">import</span> <span class="n">SQLContext</span>
<span class="n">sqlContext</span> <span class="o">=</span> <span class="n">SQLContext</span><span class="p">(</span><span class="n">sc</span><span class="p">)</span>
</pre></div>


<p>Create a DataFrame based on the content of a file:</p>
<div class="highlight"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">jsonFile</span><span class="p">(</span><span class="s">&quot;file:/path/file.json&quot;</span><span class="p">)</span>
</pre></div>


<p>Display the content of the DataFrame:</p>
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>


<p>Print the schema:</p>
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">printSchema</span><span class="p">()</span>
</pre></div>


<p>Select a column:</p>
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">select</span><span class="p">(</span><span class="s">&quot;column_name&quot;</span><span class="p">)</span>
</pre></div>


<p>Create a DataFrame with rows matching a given filter:</p>
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="n">df</span><span class="o">.</span><span class="n">column_name</span><span class="o">&gt;</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<p>Aggregate the results and count:</p>
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">groupBy</span><span class="p">(</span><span class="s">&quot;column_name&quot;</span><span class="p">)</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>


<p>Convert a RDD to a DataFrame (by inferring the schema):</p>
<div class="highlight"><pre><span class="n">df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">inferSchema</span><span class="p">(</span><span class="n">my_data</span><span class="p">)</span>
</pre></div>


<p>Register the DataFrame as a table:</p>
<div class="highlight"><pre><span class="n">df</span><span class="o">.</span><span class="n">registerTempTable</span><span class="p">(</span><span class="s">&quot;dataframe_name&quot;</span><span class="p">)</span>
</pre></div>


<p>Run a SQL Query on a DataFrame registered as a table:</p>
<div class="highlight"><pre><span class="n">rdd_from_df</span> <span class="o">=</span> <span class="n">sqlContext</span><span class="o">.</span><span class="n">sql</span><span class="p">(</span><span class="s">&quot;SELECT * FROM dataframe_name&quot;</span><span class="p">)</span>
</pre></div>


<h2>RDDs</h2>
<p>Note: RDDs are included for completeness.  In Spark 1.3, DataFrames were introduced which are recommended over RDDs.  Check out the <a href="https://databricks.com/blog/2015/02/17/introducing-dataframes-in-spark-for-large-scale-data-science.html">DataFrames announcement</a> for more info.</p>
<p>Resilient Distributed Datasets (RDDs) are the fundamental unit of data in Spark.  RDDs can be created from a file, from data in memory, or from another RDD.  RDDs are immutable.</p>
<p>There are two types of RDD operations:
<em> Actions: Returns values, data is not processed in an RDD until an action is preformed
</em> Transformations: Defines a new RDD based on the current</p>
<p>Create an RDD from the contents of a directory:</p>
<div class="highlight"><pre><span class="n">my_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;file:/path/*&quot;</span><span class="p">)</span>
</pre></div>


<p>Count the number of lines in the data:</p>
<div class="highlight"><pre><span class="n">my_data</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>


<p>Return all the elements of the dataset as an array--this is usually more useful after a filter or other operation that returns a sufficiently small subset of the data:</p>
<div class="highlight"><pre><span class="n">my_data</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>
</pre></div>


<p>Return the first 10 lines in the data:</p>
<div class="highlight"><pre><span class="n">my_data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<p>Create an RDD with lines matching the given filter:</p>
<div class="highlight"><pre><span class="n">my_data</span><span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="s">&quot;.txt&quot;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span>
</pre></div>


<p>Chain a series of commands:</p>
<div class="highlight"><pre><span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;file:/path/file.txt&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="s">&quot;.txt&quot;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">count</span><span class="p">()</span>
</pre></div>


<p>Create a new RDD mapping each line to an array of words, taking only the first word of each array:</p>
<div class="highlight"><pre><span class="n">first_words</span> <span class="o">=</span> <span class="n">my_data</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">()[</span><span class="mi">0</span><span class="p">])</span>
</pre></div>


<p>Output each word in first_words:</p>
<div class="highlight"><pre><span class="k">for</span> <span class="n">word</span> <span class="ow">in</span> <span class="n">first_words</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">word</span>
</pre></div>


<p>Save the first words to a text file:</p>
<div class="highlight"><pre><span class="n">first_words</span><span class="o">.</span><span class="n">saveAsTextFile</span><span class="p">(</span><span class="s">&quot;file:/path/file&quot;</span><span class="p">)</span>
</pre></div>


<h2>Pair RDDs</h2>
<p>Pair RDDs contain elements that are key-value pairs.  Keys and values can be any type.</p>
<p>Given a log file with the following space deilmited format: [date_time, user_id, ip_address, action], map each request to (user_id, 1):</p>
<div class="highlight"><pre><span class="n">DATE_TIME</span> <span class="o">=</span> <span class="mi">0</span>
<span class="n">USER_ID</span> <span class="o">=</span> <span class="mi">1</span>
<span class="n">IP_ADDRESS</span> <span class="o">=</span> <span class="mi">2</span>
<span class="n">ACTION</span> <span class="o">=</span> <span class="mi">3</span>

<span class="n">log_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;file:/path/*&quot;</span><span class="p">)</span>

<span class="n">user_actions</span> <span class="o">=</span> <span class="n">log_data</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">words</span><span class="p">:</span> <span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">USER_ID</span><span class="p">],</span> <span class="mi">1</span><span class="p">))</span>  \
    <span class="o">.</span><span class="n">reduceByKey</span><span class="p">(</span><span class="k">lambda</span> <span class="n">count1</span><span class="p">,</span> <span class="n">count2</span><span class="p">:</span> <span class="n">count1</span> <span class="o">+</span> <span class="n">count2</span><span class="p">)</span>
</pre></div>


<p>Show the top 5 users by count, sorted in descending order:</p>
<div class="highlight"><pre><span class="n">user_actions</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">pair</span><span class="p">:</span> <span class="p">(</span><span class="n">pair</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">pair</span><span class="p">[</span><span class="mi">1</span><span class="p">]))</span><span class="o">.</span><span class="n">sortyByKey</span><span class="p">(</span><span class="bp">False</span><span class="p">)</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span>
</pre></div>


<p>Group IP addresses by user id:</p>
<div class="highlight"><pre><span class="n">user_ips</span> <span class="o">=</span> <span class="n">log_data</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">())</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">words</span><span class="p">:</span> <span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="n">IP_ADDRESS</span><span class="p">],</span><span class="n">words</span><span class="p">[</span><span class="n">USER_ID</span><span class="p">]))</span> \
    <span class="o">.</span><span class="n">groupByKey</span><span class="p">()</span>
</pre></div>


<p>Given a user table with the following csv format: [user_id, user_info0, user_info1, ...], map each line to (user_id, [user_info...]):</p>
<div class="highlight"><pre><span class="n">user_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;file:/path/*&quot;</span><span class="p">)</span>

<span class="n">user_profile</span> <span class="o">=</span> <span class="n">user_data</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;,&#39;</span><span class="p">))</span> \
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">words</span><span class="p">:</span> <span class="p">(</span><span class="n">words</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="n">words</span><span class="p">[</span><span class="mi">1</span><span class="p">:]))</span>
</pre></div>


<p>Inner join the user_actions and user_profile RDDs:</p>
<div class="highlight"><pre><span class="n">user_actions_with_profile</span> <span class="o">=</span> <span class="n">user_actions</span><span class="o">.</span><span class="n">join</span><span class="p">(</span><span class="n">user_profile</span><span class="p">)</span>
</pre></div>


<p>Show the joined table:</p>
<div class="highlight"><pre><span class="k">for</span> <span class="p">(</span><span class="n">user_id</span><span class="p">,</span> <span class="p">(</span><span class="n">user_info</span><span class="p">,</span> <span class="n">count</span><span class="p">))</span> <span class="ow">in</span> <span class="n">user_actions_with_profiles</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">user_id</span><span class="p">,</span> <span class="n">count</span><span class="p">,</span> <span class="n">user_info</span>
</pre></div>


<h2>Running Spark on a Cluster</h2>
<p>Start the standalone cluster's Master and Worker daemons:</p>
<div class="highlight"><pre><span class="n">sudo</span> <span class="n">service</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span> <span class="n">start</span>
<span class="n">sudo</span> <span class="n">service</span> <span class="n">spark</span><span class="o">-</span><span class="n">worker</span> <span class="n">start</span>
</pre></div>


<p>Stop the standalone cluster's Master and Worker daemons:</p>
<div class="highlight"><pre><span class="n">sudo</span> <span class="n">service</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span> <span class="n">stop</span>
<span class="n">sudo</span> <span class="n">service</span> <span class="n">spark</span><span class="o">-</span><span class="n">worker</span> <span class="n">stop</span>
</pre></div>


<p>Restart the standalone cluster's Master and Worker daemons:</p>
<div class="highlight"><pre><span class="n">sudo</span> <span class="n">service</span> <span class="n">spark</span><span class="o">-</span><span class="n">master</span> <span class="n">stop</span>
<span class="n">sudo</span> <span class="n">service</span> <span class="n">spark</span><span class="o">-</span><span class="n">worker</span> <span class="n">stop</span>
</pre></div>


<p>View the Spark standalone cluster UI:</p>
<div class="highlight"><pre><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">18080</span><span class="o">//</span>
</pre></div>


<p>Start the Spark shell and connect to the cluster:</p>
<div class="highlight"><pre><span class="n">MASTER</span><span class="o">=</span><span class="n">spark</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">7077</span> <span class="n">pyspark</span>
</pre></div>


<p>Confirm you are connected to the correct master:</p>
<div class="highlight"><pre><span class="n">sc</span><span class="o">.</span><span class="n">master</span>
</pre></div>


<h2>Viewing the Spark Application UI</h2>
<p>From the following <a href="http://spark.apache.org/docs/1.2.0/monitoring.html">reference</a>:</p>
<p>Every SparkContext launches a web UI, by default on port 4040, that displays useful information about the application. This includes:</p>
<p>A list of scheduler stages and tasks
A summary of RDD sizes and memory usage
Environmental information.
Information about the running executors</p>
<p>You can access this interface by simply opening http://<driver-node>:4040 in a web browser. If multiple SparkContexts are running on the same host, they will bind to successive ports beginning with 4040 (4041, 4042, etc).</p>
<p>Note that this information is only available for the duration of the application by default. To view the web UI after the fact, set spark.eventLog.enabled to true before starting the application. This configures Spark to log Spark events that encode the information displayed in the UI to persisted storage.</p>
<div class="highlight"><pre><span class="n">http</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">4040</span><span class="o">/</span>
</pre></div>


<h2>Working with Partitions</h2>
<p>From the following <a href="http://blog.cloudera.com/blog/2014/09/how-to-translate-from-mapreduce-to-apache-spark/">reference</a>:</p>
<p>The Spark map() and flatMap() methods only operate on one input at a time, and provide no means to execute code before or after transforming a batch of values. It looks possible to simply put the setup and cleanup code before and after a call to map() in Spark:</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">dbConnection</span> <span class="o">=</span> <span class="o">...</span>
<span class="n">lines</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="o">...</span> <span class="n">dbConnection</span><span class="o">.</span><span class="n">createStatement</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="o">...</span><span class="p">)</span>
<span class="n">dbConnection</span><span class="o">.</span><span class="n">close</span><span class="p">()</span> <span class="o">//</span> <span class="n">Wrong</span><span class="err">!</span>
</pre></div>


<p>However, this fails for several reasons:</p>
<ul>
<li>It puts the object dbConnection into the map function’s closure, which requires that it be serializable (for example, by implementing java.io.Serializable). An object like a database connection is generally not serializable.</li>
<li>map() is a transformation, rather than an operation, and is lazily evaluated. The connection can’t be closed immediately here.</li>
<li>Even so, it would only close the connection on the driver, not necessarily freeing resources allocated by serialized copies.</li>
</ul>
<p>In fact, neither map() nor flatMap() is the closest counterpart to a Mapper in Spark — it’s the important mapPartitions() method. This method does not map just one value to one other value, but rather maps an Iterator of values to an Iterator of other values. It’s like a “bulk map” method. This means that the mapPartitions() function can allocate resources locally at its start, and release them when done mapping many values.</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">count_txt</span><span class="p">(</span><span class="n">partIter</span><span class="p">):</span>
    <span class="k">for</span> <span class="n">line</span> <span class="ow">in</span> <span class="n">partIter</span><span class="p">:</span>
        <span class="k">if</span> <span class="s">&quot;.txt&quot;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">:</span> <span class="n">txt_count</span> <span class="o">+=</span> <span class="mi">1</span>
    <span class="k">yield</span> <span class="p">(</span><span class="n">txt_count</span><span class="p">)</span>

<span class="n">my_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="s">&quot;file:/path/*&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">mapPartitions</span><span class="p">(</span><span class="n">count_txt</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">collect</span><span class="p">()</span>

<span class="c"># Show the partitioning</span>
<span class="k">print</span> <span class="s">&quot;Data partitions: &quot;</span><span class="p">,</span> <span class="n">my_data</span><span class="o">.</span><span class="n">toDebugString</span><span class="p">()</span>
</pre></div>


<h2>Caching RDDs</h2>
<p>Caching an RDD saves the data in memory.  Caching is a suggestion to Spark as it is memory dependent.</p>
<p>By default, every RDD operation executes the entire lineage.  Caching can boost performance for datasets that are likely to be used by saving this expensive recomputation and is ideal for iterative algorithms or machine learning.</p>
<ul>
<li>cache() stores data in memory</li>
<li>persist() stores data in MEMORY_ONLY, MEMORY_AND_DISK (spill to disk), and  DISK_ONLY</li>
</ul>
<p>Disk memory is stored on the node, not on HDFS.</p>
<p>Replication is possible by using MEMORY_ONLY_2, MEMORY_AND_DISK_2, etc.  If a cached partition becomes unavailable, Spark recomputes the partition through the lineage.</p>
<p>Serialization is possible with MEMORY_ONLY_SER and MEMORY_AND_DISK_SER.  This is more space efficient but less time efficient, as it uses Java serialization by default.</p>
<div class="highlight"><pre><span class="c"># Cache RDD to memory</span>
<span class="n">my_data</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>

<span class="c"># Persist RDD to both memory and disk (if memory is not enough), with replication of 2</span>
<span class="n">my_data</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">MEMORY_AND_DISK_2</span><span class="p">)</span>

<span class="c"># Unpersist RDD, removing it from memory and disk</span>
<span class="n">my_data</span><span class="o">.</span><span class="n">unpersist</span><span class="p">()</span>

<span class="c"># Change the persistence level after unpersist</span>
<span class="n">my_data</span><span class="o">.</span><span class="n">persist</span><span class="p">(</span><span class="n">MEMORY_AND_DISK</span><span class="p">)</span>
</pre></div>


<h2>Checkpointing RDDs</h2>
<p>Caching maintains RDD lineage, providing resilience.  If the lineage is very long, it is possible to get a stack overflow.</p>
<p>Checkpointing saves the data to HDFS, which provide fault tolerant storage across nodes.  HDFS is not as fast as local storage for both reading and writing.  Checkpointing is good for long lineages and for very large data sets that might not fit on local storage.  Checkpointing removes lineage.</p>
<p>Create a checkpoint and perform an action by calling count() to materialize the checkpoint and save it to the checkpoint file:</p>
<div class="highlight"><pre><span class="c"># Enable checkpointing by setting the checkpoint directory,</span>
<span class="c"># which will contain all checkpoints for the given data:</span>
<span class="n">sc</span><span class="o">.</span><span class="n">setCheckpointDir</span><span class="p">(</span><span class="s">&quot;checkpoints&quot;</span><span class="p">)</span>

<span class="n">my_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">parallelize</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span><span class="mi">2</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span><span class="mi">4</span><span class="p">,</span><span class="mi">5</span><span class="p">])</span>

<span class="c"># Long loop that may cause a stack overflow</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">1000</span><span class="p">):</span>
    <span class="n">my_data</span> <span class="o">=</span> <span class="n">mydata</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">myInt</span><span class="p">:</span> <span class="n">myInt</span> <span class="o">+</span> <span class="mi">1</span><span class="p">)</span>

    <span class="k">if</span> <span class="n">i</span> <span class="o">%</span> <span class="mi">10</span> <span class="o">==</span> <span class="mi">0</span><span class="p">:</span>
        <span class="n">my_data</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">()</span>
        <span class="n">my_data</span><span class="o">.</span><span class="n">count</span><span class="p">()</span>

<span class="n">my_data</span><span class="o">.</span><span class="n">collect</span><span class="p">()</span>

<span class="c"># Display the lineage</span>
<span class="k">for</span> <span class="n">rddstring</span> <span class="ow">in</span> <span class="n">my_data</span><span class="o">.</span><span class="n">toDebugString</span><span class="p">()</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39;</span><span class="se">\n</span><span class="s">&#39;</span><span class="p">):</span>
    <span class="k">print</span> <span class="n">rddstring</span><span class="o">.</span><span class="n">strip</span><span class="p">()</span>
</pre></div>


<h2>Writing and Running a Spark Application</h2>
<p>Create a Spark application to count the number of text files:</p>
<div class="highlight"><pre><span class="kn">import</span> <span class="nn">sys</span>
<span class="kn">from</span> <span class="nn">pyspark</span> <span class="kn">import</span> <span class="n">SparkContext</span>

<span class="k">if</span> <span class="n">__name__</span> <span class="o">==</span> <span class="s">&quot;__main__&quot;</span><span class="p">:</span>
    <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">)</span> <span class="o">&lt;</span> <span class="mi">2</span><span class="p">:</span>
        <span class="k">print</span> <span class="o">&gt;&gt;</span> <span class="n">sys</span><span class="o">.</span><span class="n">stderr</span><span class="p">,</span> <span class="s">&quot;Usage: App Name &lt;file&gt;&quot;</span>
        <span class="nb">exit</span><span class="p">(</span><span class="o">-</span><span class="mi">1</span><span class="p">)</span>

    <span class="n">count_text_files</span><span class="p">()</span>

<span class="k">def</span> <span class="nf">count_text_files</span><span class="p">():</span>
    <span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">()</span>
    <span class="n">logfile</span> <span class="o">=</span> <span class="n">sys</span><span class="o">.</span><span class="n">argv</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span>
    <span class="n">text_files_count</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">logfile</span><span class="p">)</span>
        <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="s">&#39;.txt&#39;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">)</span>
    <span class="n">text_files_count</span><span class="o">.</span><span class="n">cache</span><span class="p">()</span>
    <span class="k">print</span><span class="p">(</span><span class="s">&quot;Number of text files: &quot;</span><span class="p">,</span> <span class="n">text_files_count</span><span class="o">.</span><span class="n">count</span><span class="p">())</span>
</pre></div>


<p>Submit the script to Spark for processing:</p>
<div class="highlight"><pre><span class="n">spark</span><span class="o">-</span><span class="n">submit</span> <span class="o">--</span><span class="n">properties</span><span class="o">-</span><span class="nb">file</span> <span class="nb">dir</span><span class="o">/</span><span class="n">myspark</span><span class="o">.</span><span class="n">conf</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">data</span><span class="o">/*</span>
</pre></div>


<h2>Configuring Spark Applications</h2>
<p>Run a Spark app and set the configuration options in the command line:</p>
<div class="highlight"><pre><span class="n">spark</span><span class="o">-</span><span class="n">submit</span> <span class="o">--</span><span class="n">master</span> <span class="n">spark</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">7077</span> <span class="o">--</span><span class="n">name</span> <span class="s">&#39;App Name&#39;</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">data</span><span class="o">/*</span>
</pre></div>


<p>Configure spark.conf:</p>
<div class="highlight"><pre><span class="n">spark</span><span class="o">.</span><span class="n">app</span><span class="o">.</span><span class="n">name</span>  <span class="n">App</span> <span class="n">Name</span>
<span class="n">spark</span><span class="o">.</span><span class="n">ui</span><span class="o">.</span><span class="n">port</span>   <span class="mi">4141</span>
<span class="n">spark</span><span class="o">.</span><span class="n">master</span>    <span class="n">spark</span><span class="p">:</span><span class="o">//</span><span class="n">localhost</span><span class="p">:</span><span class="mi">7077</span>
</pre></div>


<p>Run a Spark app and set the configuration options through spark.conf:</p>
<div class="highlight"><pre><span class="n">spark</span><span class="o">-</span><span class="n">submit</span> <span class="o">--</span><span class="n">properties</span><span class="o">-</span><span class="nb">file</span> <span class="n">spark</span><span class="o">.</span><span class="n">conf</span> <span class="n">script</span><span class="o">.</span><span class="n">py</span> <span class="n">data</span><span class="o">/*</span>
</pre></div>


<p>Set the config options programmatically:</p>
<div class="highlight"><pre><span class="n">sconf</span> <span class="o">=</span> <span class="n">SparkConf</span><span class="p">()</span> \
    <span class="o">.</span><span class="n">setAppName</span><span class="p">(</span><span class="s">&quot;Word Count&quot;</span><span class="p">)</span> \
    <span class="o">.</span><span class="n">set</span><span class="p">(</span><span class="s">&quot;spark.ui.port&quot;</span><span class="p">,</span><span class="s">&quot;4141&quot;</span><span class="p">)</span>
<span class="n">sc</span> <span class="o">=</span> <span class="n">SparkContext</span><span class="p">(</span><span class="n">conf</span><span class="o">=</span><span class="n">sconf</span><span class="p">)</span>
</pre></div>


<p>Set logging levels located in the following file, or place a copy in your pwd:</p>
<div class="highlight"><pre><span class="err">$</span><span class="n">SPARK_HOME</span><span class="o">/</span><span class="n">conf</span><span class="o">/</span><span class="n">log4j</span><span class="o">.</span><span class="n">properties</span><span class="o">.</span><span class="n">template</span>
</pre></div>


<h2>Streaming</h2>
<p>Start the Spark Shell locally with at least two threads (need a minimum of two threads for streaming, one for receiving, one for processing):</p>
<div class="highlight"><pre><span class="n">spark</span><span class="o">-</span><span class="n">shell</span> <span class="o">--</span><span class="n">master</span> <span class="n">local</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span>
</pre></div>


<p>Create a StreamingContext (similar to SparkContext in core Spark) with a batch duration of 1 second:</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">ssc</span> <span class="o">=</span> <span class="n">new</span> <span class="n">StreamingContext</span><span class="p">(</span><span class="n">new</span> <span class="n">SparkConf</span><span class="p">(),</span> <span class="n">Seconds</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
<span class="n">val</span> <span class="n">my_stream</span> <span class="o">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="p">(</span><span class="n">hostname</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>
</pre></div>


<p>Get a DStream from a streaming data source (text from a socket):</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">logs</span> <span class="o">=</span> <span class="n">ssc</span><span class="o">.</span><span class="n">socketTextStream</span><span class="p">(</span><span class="n">hostname</span><span class="p">,</span> <span class="n">port</span><span class="p">)</span>
</pre></div>


<p>DStreams support regular transformations such as map, flatMap, and filter, and pair transformations such as reduceByKey, groupByKey, and joinByKey.</p>
<p>Apply a DStream operation to each batch of RDDs (count up requests by user id, reduce by key to get the count):</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">requests</span> <span class="o">=</span> <span class="n">my_stream</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">line</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&quot; &quot;</span><span class="p">)(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="o">.</span><span class="n">reduceByKey</span><span class="p">((</span><span class="n">x</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">)</span>
</pre></div>


<p>The transform(function) method creates a new DStream by executing the input function on the RDDs.</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">sorted_requests</span> <span class="o">=</span> <span class="n">requests</span>
    <span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">pair</span> <span class="o">=&gt;</span> <span class="n">pair</span><span class="o">.</span><span class="n">swap</span><span class="p">)</span>
    <span class="o">.</span><span class="n">transform</span><span class="p">(</span><span class="n">rdd</span> <span class="o">=&gt;</span> <span class="n">rdd</span><span class="o">.</span><span class="n">sortByKey</span><span class="p">(</span><span class="n">false</span><span class="p">))</span>
</pre></div>


<p>foreachRDD(function) performs a function on each RDD in the DStream (map is like a shortcut not requiring you to get the RDD first before doing an operation):</p>
<div class="highlight"><pre><span class="n">sorted_requests</span><span class="o">.</span><span class="n">foreachRDD</span><span class="p">((</span><span class="n">rdd</span><span class="p">,</span> <span class="n">time</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="n">println</span><span class="p">(</span><span class="s">&quot;Top users @ &quot;</span> <span class="o">+</span> <span class="n">time</span><span class="p">)</span>
    <span class="n">rdd</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">5</span><span class="p">)</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span>
    <span class="n">pair</span> <span class="o">=&gt;</span> <span class="n">printf</span><span class="p">(</span><span class="s">&quot;User: </span><span class="si">%s</span><span class="s"> (</span><span class="si">%s</span><span class="s">)</span><span class="se">\n</span><span class="s">&quot;</span><span class="p">,</span> <span class="n">pair</span><span class="o">.</span><span class="n">_2</span><span class="p">,</span> <span class="n">pair</span><span class="o">.</span><span class="n">_1</span><span class="p">))</span>
<span class="p">}</span>
</pre></div>


<p>Save the DStream result part files with the given folder prefix, the actual folder will be /dir/requests-timestamp0/:</p>
<div class="highlight"><pre><span class="n">requests</span><span class="o">.</span><span class="n">saveAsTextFiles</span><span class="p">(</span><span class="s">&quot;/dir/requests&quot;</span><span class="p">)</span>
</pre></div>


<p>Start the execution of all DStreams:</p>
<div class="highlight"><pre><span class="n">ssc</span><span class="o">.</span><span class="n">start</span><span class="p">()</span>
</pre></div>


<p>Wait for all background threads to complete before ending the main thread:</p>
<div class="highlight"><pre><span class="n">ssc</span><span class="o">.</span><span class="n">awaitTermination</span><span class="p">()</span>
</pre></div>


<h2>Streaming with States</h2>
<p>Enable checkpointing to prevent infinite lineages:</p>
<div class="highlight"><pre><span class="n">ssc</span><span class="o">.</span><span class="n">checkpoint</span><span class="p">(</span><span class="s">&quot;dir&quot;</span><span class="p">)</span>
</pre></div>


<p>Compute a DStream based on the previous states plus the current state:</p>
<div class="highlight"><pre><span class="k">def</span> <span class="nf">updateCount</span> <span class="o">=</span> <span class="p">(</span><span class="n">newCounts</span><span class="p">:</span> <span class="n">Seq</span><span class="p">[</span><span class="n">Int</span><span class="p">],</span> <span class="n">state</span><span class="p">:</span> <span class="n">Option</span><span class="p">[</span><span class="n">Int</span><span class="p">])</span> <span class="o">=&gt;</span> <span class="p">{</span>
    <span class="n">val</span> <span class="n">newCount</span> <span class="o">=</span> <span class="n">newCounts</span><span class="o">.</span><span class="n">foldLeft</span><span class="p">(</span><span class="mi">0</span><span class="p">)(</span><span class="n">_</span> <span class="o">+</span> <span class="n">_</span><span class="p">)</span>
    <span class="n">val</span> <span class="n">previousCount</span> <span class="o">=</span> <span class="n">state</span><span class="o">.</span><span class="n">getOrElse</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">Some</span><span class="p">(</span><span class="n">newCount</span> <span class="o">+</span> <span class="n">previousCount</span><span class="p">)</span>
<span class="p">}</span>

<span class="n">val</span> <span class="n">totalUserreqs</span> <span class="o">=</span> <span class="n">userreqs</span><span class="o">.</span><span class="n">updateStateByKey</span><span class="p">(</span><span class="n">updateCount</span><span class="p">)</span>
</pre></div>


<p>Compute a DStream based Sliding window, every 30 seconds, count requests by user over the last 5 minutes:</p>
<div class="highlight"><pre><span class="n">val</span> <span class="n">reqcountsByWindow</span> <span class="o">=</span> <span class="n">logs</span><span class="o">.</span><span class="n">map</span><span class="p">(</span><span class="n">line</span> <span class="o">=&gt;</span> <span class="p">(</span><span class="n">line</span><span class="o">.</span><span class="n">split</span><span class="p">(</span><span class="s">&#39; &#39;</span><span class="p">)(</span><span class="mi">2</span><span class="p">),</span> <span class="mi">1</span><span class="p">))</span>
    <span class="o">.</span><span class="n">reduceByKeyAndWindow</span><span class="p">((</span><span class="n">x</span><span class="p">:</span> <span class="n">Int</span><span class="p">,</span> <span class="n">y</span><span class="p">:</span> <span class="n">Int</span><span class="p">)</span> <span class="o">=&gt;</span> <span class="n">x</span> <span class="o">+</span> <span class="n">y</span><span class="p">,</span> <span class="n">Minutes</span><span class="p">(</span><span class="mi">5</span><span class="p">),</span> <span class="n">Seconds</span><span class="p">(</span><span class="mi">30</span><span class="p">))</span>
</pre></div>


<p>Collect statistics with the StreamingListener API:</p>
<div class="highlight"><pre><span class="o">//</span> <span class="n">define</span> <span class="n">listener</span>
<span class="k">class</span> <span class="nc">MyListener</span> <span class="n">extends</span> <span class="n">StreamingListener</span> <span class="p">{</span>
  <span class="n">override</span> <span class="k">def</span> <span class="nf">onReceiverStopped</span><span class="p">(</span><span class="o">...</span><span class="p">)</span> <span class="p">{</span>
    <span class="n">streamingContext</span><span class="o">.</span><span class="n">stop</span><span class="p">()</span>
  <span class="p">}</span>
<span class="p">}</span>

<span class="o">//</span> <span class="n">attach</span> <span class="n">listener</span>
<span class="n">streamingContext</span><span class="o">.</span> <span class="n">addStreamingListener</span><span class="p">(</span><span class="n">new</span> <span class="n">MyListener</span><span class="p">())</span>
</pre></div>


<h2>Broadcast Variables</h2>
<p>Read in list of items to broadcast from a local file:</p>
<div class="highlight"><pre><span class="n">broadcast_file</span> <span class="o">=</span> <span class="s">&quot;broadcast.txt&quot;</span>
<span class="n">broadcast_list</span> <span class="o">=</span> <span class="nb">list</span><span class="p">(</span><span class="nb">map</span><span class="p">(</span><span class="k">lambda</span> <span class="n">l</span><span class="p">:</span> <span class="n">l</span><span class="o">.</span><span class="n">strip</span><span class="p">(),</span> <span class="nb">open</span><span class="p">(</span><span class="n">broadcast_file</span><span class="p">)))</span>
</pre></div>


<p>Broadcast the target list to all workers:</p>
<div class="highlight"><pre><span class="n">broadcast_list_sc</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">broadcast</span><span class="p">(</span><span class="n">broadcast_list</span><span class="p">)</span>
</pre></div>


<p>Filter based on the broadcast list:</p>
<div class="highlight"><pre><span class="n">log_file</span> <span class="o">=</span> <span class="s">&quot;hdfs://localhost/user/logs/*&quot;</span>
<span class="n">filtered_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">log_file</span><span class="p">)</span>\
    <span class="o">.</span><span class="n">filter</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="nb">any</span><span class="p">(</span><span class="n">item</span> <span class="ow">in</span> <span class="n">line</span> <span class="k">for</span> <span class="n">item</span> <span class="ow">in</span> <span class="n">broadcast_list_sc</span><span class="o">.</span><span class="n">value</span><span class="p">))</span>

<span class="n">filtered_data</span><span class="o">.</span><span class="n">take</span><span class="p">(</span><span class="mi">10</span><span class="p">)</span>
</pre></div>


<h2>Accumulators</h2>
<p>Create an accumulator:</p>
<div class="highlight"><pre><span class="n">txt_count</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
</pre></div>


<p>Count the number of txt files in the RDD:</p>
<div class="highlight"><pre><span class="n">my_data</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">filePath</span><span class="p">)</span>
<span class="n">my_data</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="k">if</span> <span class="s">&#39;.txt&#39;</span> <span class="ow">in</span> <span class="n">line</span><span class="p">:</span> <span class="n">txt_count</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">))</span>
</pre></div>


<p>Count the number of file types encountered:</p>
<div class="highlight"><pre><span class="n">jpg_count</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">html_count</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>
<span class="n">css_count</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">accumulator</span><span class="p">(</span><span class="mi">0</span><span class="p">)</span>

<span class="k">def</span> <span class="nf">countFileType</span><span class="p">(</span><span class="n">s</span><span class="p">):</span>
    <span class="k">if</span> <span class="s">&#39;.jpg&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span> <span class="n">jpg_count</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">&#39;.html&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span> <span class="n">html_count</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>
    <span class="k">elif</span> <span class="s">&#39;.css&#39;</span> <span class="ow">in</span> <span class="n">s</span><span class="p">:</span> <span class="n">css_count</span><span class="o">.</span><span class="n">add</span><span class="p">(</span><span class="mi">1</span><span class="p">)</span>

<span class="n">filename</span><span class="o">=</span><span class="s">&quot;hdfs://logs/*&quot;</span>

<span class="n">logs</span> <span class="o">=</span> <span class="n">sc</span><span class="o">.</span><span class="n">textFile</span><span class="p">(</span><span class="n">filename</span><span class="p">)</span>
<span class="n">logs</span><span class="o">.</span><span class="n">foreach</span><span class="p">(</span><span class="k">lambda</span> <span class="n">line</span><span class="p">:</span> <span class="n">countFileType</span><span class="p">(</span><span class="n">line</span><span class="p">))</span>

<span class="k">print</span>  <span class="s">&#39;File Type Totals:&#39;</span>
<span class="k">print</span> <span class="s">&#39;.css files: &#39;</span><span class="p">,</span> <span class="n">css_count</span><span class="o">.</span><span class="n">value</span>
<span class="k">print</span> <span class="s">&#39;.html files: &#39;</span><span class="p">,</span> <span class="n">html_count</span><span class="o">.</span><span class="n">value</span>
<span class="k">print</span> <span class="s">&#39;.jpg files: &#39;</span><span class="p">,</span> <span class="n">jpg_count</span><span class="o">.</span><span class="n">value</span>
</pre></div>
            </div>
            <hr class="featurette-divider">
            <!-- /.entry-content -->
        </article>
    </section>

        </div>
    </div>
</div>
    <footer>
        <div class="container">
            <div class="row">
                <div class="col-md-12 text-left">
                    <span class="copyright">Copyright &copy; Donne Martin 2014-Present</span>
                </div>
            </div>
        </div>
    </footer>
<script src="./theme/js/jquery.min.js"></script>

<!-- Include all compiled plugins (below), or include individual files as needed -->
<script src="./theme/js/bootstrap.min.js"></script>

<!-- Enable responsive features in IE8 with Respond.js (https://github.com/scottjehl/Respond) -->
<script src="./theme/js/respond.min.js"></script>


<!-- Plugin JavaScript -->
<script src="http://cdnjs.cloudflare.com/ajax/libs/jquery-easing/1.3/jquery.easing.min.js"></script>
<script src="./theme/js/classie.js"></script>
<script src="./theme/js/cbpAnimatedHeader.js"></script>

<!-- Custom Theme JavaScript -->
<script src="./theme/js/agency.js"></script>

<!-- Google Analytics Universal -->
<script>
  (function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
  (i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
  m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
  })(window,document,'script','//www.google-analytics.com/analytics.js','ga');

  ga('create', 'UA-54747412-1', 'auto');
  ga('send', 'pageview');

</script>
<!-- End Google Analytics Universal Code -->
</body>
</html>